{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50479ccf-9dc8-4a0b-b3c9-6685bda6fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as t\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import jiwer\n",
    "import cv2\n",
    "from typing import List\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from datasets import load_dataset, Audio, Dataset\n",
    "import random \n",
    "from torchvision.io import read_image\n",
    "import time\n",
    "import torchaudio\n",
    "from transformers import (Wav2Vec2ForCTC, Wav2Vec2Processor, Trainer, TrainingArguments,\n",
    "                          Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, DataCollatorWithPadding, AutoConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a433bd0-16b5-47e2-a934-b1fda56a8753",
   "metadata": {},
   "source": [
    "## DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3811a15f-e8e9-4e7d-a621-fba1891d3c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 132\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a7d8204-2819-4b35-8adb-9bb1f2a68cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_audio_path = 'G:/data/vivos/train/waves'\n",
    "train_prompts_path = 'G:/data/vivos/train/prompts.txt'\n",
    "train_genders_path = 'G:/data/vivos/train/genders.txt'\n",
    "test_audio_path = 'G:/data/vivos/test/waves'\n",
    "test_prompts_path = 'G:/data/vivos/test/prompts.txt'\n",
    "test_genders_path = 'G:/data/vivos/test/genders.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00e86e28-dfab-424b-99ee-e1d7290b9e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompts(prompts_path):\n",
    "    transcripts = []\n",
    "    with open(prompts_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            id, text = line.strip().split(' ', 1)\n",
    "            transcripts.append({'id': id, 'text': text.lower()})\n",
    "    return pd.DataFrame(transcripts)\n",
    "\n",
    "\n",
    "train_transcripts = load_prompts(train_prompts_path)\n",
    "test_transcripts = load_prompts(test_prompts_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78c4f003-ebfa-4b9a-ac19-114c6ef144fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_path(audio_base_path, audio_id):\n",
    "    speaker = audio_id.split('_')[0]\n",
    "    return os.path.join(audio_base_path, speaker, audio_id + '.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "717b6181-4303-403c-8d52-56d82ada6a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transcripts['audio'] = train_transcripts['id'].apply(lambda x: get_audio_path(train_audio_path, x))\n",
    "test_transcripts['audio'] = test_transcripts['id'].apply(lambda x: get_audio_path(test_audio_path, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f8e6e15-e95e-4000-bb53-0f2a27d267b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"“%‘”�]'\n",
    "\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower()\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff3d0c39-63e1-4b49-922f-df762b6308da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transcripts = train_transcripts.apply(remove_special_characters, axis=1)\n",
    "test_transcripts = test_transcripts.apply(remove_special_characters, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4c9c7d-adef-4cf2-bc84-0750149c5457",
   "metadata": {},
   "source": [
    "## DATA PIPELINE AND TRAIN TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "723c22a3-6d78-47c0-94c9-15ad4d485a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_transcripts)\n",
    "test_dataset = Dataset.from_pandas(test_transcripts)\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "test_dataset = test_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "train_valid_split = train_dataset.train_test_split(test_size=0.1, seed=42)  # 90% train, 10% validation\n",
    "train_dataset = train_valid_split[\"train\"]\n",
    "valid_dataset = train_valid_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81c4db7f-a786-4cfd-a2f9-cdb44e9a48e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\"nguyenvulebinh/wav2vec2-base-vietnamese-250h\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9dfe61a2-23f2-41fa-8484-a27e51b97994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Size: 10494\n",
      "Validation Set Size: 1166\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch, preprocessor, sampling_rate=16000):\n",
    "    audio_tensors = [torch.tensor(item[\"audio\"][\"array\"], dtype=torch.float32) for item in batch]\n",
    "    text_data = [item[\"text\"] for item in batch]\n",
    "\n",
    "    # Pad audio data\n",
    "    audio_padded = torch.nn.utils.rnn.pad_sequence(audio_tensors, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Tokenize text data (convert to input IDs with padding)\n",
    "    text_encodings = preprocessor(text=text_data, padding=True, return_tensors=\"pt\")\n",
    "    input_ids = text_encodings.input_ids\n",
    "\n",
    "    # Ensure that the audio field is passed correctly\n",
    "    audio_encodings = preprocessor(audio=audio_padded, sampling_rate=sampling_rate, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    return {\n",
    "        \"audio\": audio_encodings.input_values,\n",
    "        \"input_ids\": input_ids,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: collate_fn(batch, processor, sampling_rate=16000),\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda batch: collate_fn(batch, processor, sampling_rate=16000),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Check the size of the datasets\n",
    "print(f\"Training Set Size: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation Set Size: {len(valid_loader.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11865fcb-3af5-4ef1-bbbe-dac1294c7a7c",
   "metadata": {},
   "source": [
    "## MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "441f6f9c-9c02-4eff-b24c-699f91f32dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "class VietLip(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, feature_size, hidden_size,\n",
    "                num_layers, dropout, bidirectional, device='cpu'):\n",
    "        super(VietLip, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.directions = 2 if bidirectional else 1\n",
    "        self.device = device\n",
    "        self.layernorm = nn.LayerNorm(feature_size)\n",
    "        self.lstm = nn.LSTM(input_size=feature_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, dropout=dropout,\n",
    "                            bidirectional=bidirectional)\n",
    "        self.classifier = nn.Linear(hidden_size*self.directions, num_classes)\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        n, d, hs = self.num_layers, self.directions, self.hidden_size\n",
    "        return (torch.zeros(n*d, batch_size, hs).to(self.device),\n",
    "                torch.zeros(n*d, batch_size, hs).to(self.device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape => seq_len, batch, feature\n",
    "        x = self.layernorm(x)\n",
    "        hidden = self._init_hidden(x.size()[1])\n",
    "        out, (hn, cn) = self.lstm(x, hidden)\n",
    "        out = self.classifier(hn)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f0eef5-828d-4e57-a056-5d3f740a1bef",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab25367d-e47a-48dc-997c-bc79ad19e13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torchaudio.transforms import Resample\n",
    "\n",
    "class CTCLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CTCLoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_true, y_pred):\n",
    "        batch_size = y_true.size(0)\n",
    "        input_length = y_pred.size(1)\n",
    "        label_length = y_true.size(1)\n",
    "\n",
    "        # Create length tensors\n",
    "        input_length = torch.full((batch_size,), input_length, dtype=torch.long)\n",
    "        label_length = torch.full((batch_size,), label_length, dtype=torch.long)\n",
    "\n",
    "        loss = nn.CTCLoss()(y_pred.log_softmax(2), y_true, input_length, label_length)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "495660dd-d881-4369-bbc4-8e220673aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_device():\n",
    "    if torch.cuda.is_available():\n",
    "        dev = \"cuda\"\n",
    "    else:\n",
    "        dev = \"cpu\"\n",
    "    return torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bdbb72d1-1074-4fdf-ba24-8d68c1012b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, loss, wer, filename=\"best_model.pth\"):\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"loss\": loss,\n",
    "        \"wer\": wer,\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Checkpoint saved: {filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f0bda1f-ac43-4878-bfa8-31e9e7d6977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, preprocessor, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = batch[\"audio\"][\"array\"]\n",
    "            targets = batch[\"text\"]\n",
    "\n",
    "            # Preprocess audio\n",
    "            inputs = [preprocessor(inputs[i], sampling_rate=16000).input_values[0] for i in range(len(inputs))]\n",
    "            inputs = torch.tensor(inputs).to(device).unsqueeze(1)  # Add batch and channel dims\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.permute(1, 0, 2)  # [seq_len, batch_size, num_classes]\n",
    "\n",
    "            # Compute loss (optional during evaluation)\n",
    "            input_lengths = torch.full((outputs.size(1),), outputs.size(0), dtype=torch.long)\n",
    "            target_lengths = torch.tensor([len(t) for t in targets], dtype=torch.long)\n",
    "            targets_encoded = preprocessor(targets, padding=True, return_tensors=\"pt\").input_ids\n",
    "            targets_encoded = targets_encoded.to(device)\n",
    "\n",
    "            loss = criterion(outputs, targets_encoded, input_lengths, target_lengths)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Decode predictions for WER\n",
    "            predicted_indices = outputs.argmax(dim=2).cpu().numpy()\n",
    "            all_predictions.extend(predicted_indices)\n",
    "            all_targets.extend(targets)\n",
    "\n",
    "    # Compute WER\n",
    "    wer = calculate_wer(all_predictions, all_targets)\n",
    "    return total_loss / len(dataloader), wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "02104025-5a61-4aa2-bbd4-385962f1872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    preprocessor,\n",
    "    num_epochs,\n",
    "    device,\n",
    "    example_callback=None,\n",
    "    checkpoint_path=\"best_model.pth\",\n",
    "):\n",
    "    lr_scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: 1 if epoch < 30 else 0.1)\n",
    "    best_wer = float(\"inf\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            \n",
    "            inputs = batch[\"audio\"]\n",
    "            targets = batch[\"text\"]\n",
    "\n",
    "            # Preprocess audio\n",
    "            inputs = [preprocessor(inputs[i], sampling_rate=16000).input_values[0] for i in range(len(inputs))]\n",
    "            inputs = torch.tensor(inputs).to(device).unsqueeze(1)  # Add batch and channel dims\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.permute(1, 0, 2)  # [seq_len, batch_size, num_classes]\n",
    "\n",
    "            # Compute loss\n",
    "            input_lengths = torch.full((outputs.size(1),), outputs.size(0), dtype=torch.long)\n",
    "            target_lengths = torch.tensor([len(t) for t in targets], dtype=torch.long)\n",
    "            targets_encoded = preprocessor(targets, padding=True, return_tensors=\"pt\").input_ids\n",
    "            targets_encoded = targets_encoded.to(device)\n",
    "\n",
    "            loss = criterion(outputs, targets_encoded, input_lengths, target_lengths)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        valid_loss, wer = evaluate_model(model, valid_loader, preprocessor, criterion, device)\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}] | Train Loss: {running_loss / len(train_loader):.4f} | \"\n",
    "              f\"Validation Loss: {valid_loss:.4f} | WER: {wer:.2f}\")\n",
    "\n",
    "        # Save the best model based on WER\n",
    "        if wer < best_wer:\n",
    "            best_wer = wer\n",
    "            save_checkpoint(model, optimizer, epoch + 1, valid_loss, wer, checkpoint_path)\n",
    "\n",
    "        # Example callback\n",
    "        if example_callback:\n",
    "            example_callback.on_epoch_end(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b4980ae6-7d42-4c77-8c49-a4ba3d0bde29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wer(predictions, references):\n",
    "    \"\"\"Computes WER using the jiwer library.\"\"\"\n",
    "    pred_texts = [\"\".join(map(str, decode_predictions(pred))) for pred in predictions]\n",
    "    ref_texts = [\"\".join(map(str, decode_predictions(ref))) for ref in references]\n",
    "    wer = jiwer.wer(ref_texts, pred_texts)\n",
    "    return wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4bf8a232-9a9c-4c95-a88f-49b1f19eb72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = set_device()\n",
    "model = VietLip(1, 40, 256, 1, 0.1,  False).to(device)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = CTCLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "125c0aa4-01b9-4209-8dbe-741949988e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.0110e-05,  2.0110e-05,  2.0110e-05,  ...,  5.4945e-03,\n",
      "          -1.6222e-03, -1.0381e-02],\n",
      "         [-5.4724e-02, -2.5235e-01, -2.0965e-01,  ...,  2.0110e-05,\n",
      "           2.0110e-05,  2.0110e-05],\n",
      "         [ 2.0110e-05, -5.2733e-04,  5.6755e-04,  ...,  2.0110e-05,\n",
      "           2.0110e-05,  2.0110e-05],\n",
      "         ...,\n",
      "         [ 2.0110e-05, -5.2733e-04,  5.6755e-04,  ...,  2.0110e-05,\n",
      "           2.0110e-05,  2.0110e-05],\n",
      "         [ 2.0110e-05,  5.6755e-04,  2.0110e-05,  ...,  2.0110e-05,\n",
      "           2.0110e-05,  2.0110e-05],\n",
      "         [ 2.0110e-05,  2.0110e-05, -5.2733e-04,  ...,  2.0110e-05,\n",
      "           2.0110e-05,  2.0110e-05]]])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_model(\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      3\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m      4\u001b[0m     valid_loader\u001b[38;5;241m=\u001b[39mvalid_loader,\n\u001b[0;32m      5\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m      6\u001b[0m     criterion\u001b[38;5;241m=\u001b[39mcriterion,\n\u001b[0;32m      7\u001b[0m     preprocessor\u001b[38;5;241m=\u001b[39mprocessor,\n\u001b[0;32m      8\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m      9\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m     10\u001b[0m     checkpoint_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvietlip_best_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m )\n",
      "Cell \u001b[1;32mIn[47], line 22\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, valid_loader, optimizer, criterion, preprocessor, num_epochs, device, example_callback, checkpoint_path)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m---> 22\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     23\u001b[0m     targets \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Preprocess audio\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 3"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    preprocessor=processor,\n",
    "    num_epochs=50,\n",
    "    device=device,\n",
    "    checkpoint_path=\"vietlip_best_model.pth\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "53a5d8bc-de66-4317-bc1d-0dc1cda53333",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'batch' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ed9ac4-cf8a-417d-932d-9fe53e0bacd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
